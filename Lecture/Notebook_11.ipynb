{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"font-family:Impact,Arial;font-size:70px;\">Statistical data analysis II</h1>\n",
    "<h2 style=\"font-family:Arial;\">Matias Quiroz</h2>\n",
    "<p><small> School of Mathematical &amp; Physical Sciences<br>\n",
    "University of Technology Sydney\n",
    "</small></p>\n",
    "<p>\n",
    "<a href=\"mailto:matias.quiroz@uts.edu.au\" target=\"_blank\">\n",
    "<small><font color=MediumVioletRed>matias.quiroz@uts.edu.au</font></small></a>\n",
    "</p>\n",
    "<hr style=\"height:5px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#F2FBEF;\">\n",
    "<h2><font color=#04B404>After completing this notebook you should:</font></h2>\n",
    "<br>\n",
    "<ul>\n",
    "<li> Have a basic understanding of how to fit simple regression and classification models in Python using packages. </li><br>\n",
    "<li> Have a basic knowledge of how to implement the statistical methods underlying the packages yourself. </li><br>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "# 1. Logistic regression without `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regression examples in Notebook 10, the parameter estimates could be obtained in closed form using ordinary lease squares (OLS). This is not possible for the logistic regression model (and most other models) and one often needs to resort to numerical optimisation. This is what the function `LogisticRegression()` we used last week does under the hood and we now demonstrate how this can be implemented.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;We first need to decide on an objective function to optimise. In the regression case, a suitable objective function was the error sum of squares\n",
    "\n",
    "$$(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}),$$\n",
    "\n",
    "however, this is not suitable for classification problems as $y$ is categorical. At first glance, it seems reasonable to minimise the misclassification rate \n",
    "\n",
    "$$\\mathrm{MCR}(\\boldsymbol{\\beta}) = \\frac{1}{n_{\\mathrm{test}}}\\sum_{i \\in \\mathcal{T}_{\\mathrm{test}}} \\mathbb{1}(y_i \\neq \\widehat{y}_i(\\boldsymbol{\\beta})),$$\n",
    "\n",
    "where the dependence on $\\boldsymbol{\\beta}$ is emphasised. Optimising a function benefits greatly from using the gradient of the function and since the misclassification rate is discontinuous in $\\boldsymbol{\\beta}$, it is not a good choice since it is not differentiable. A common approach is to instead minimise the negative log-likelihood function (which is equivalent to maximise the positive log-likelihood function) which we now turn to. Intuitively, when maximising the log-likelihood, one finds the set of parameter values that are most plausible (in the sense that they give the highest likelihood) for the given sample.\n",
    "\n",
    "The log-likelihood based on $\\mathbf{y}=(y_1, \\dots, y_n)^\\top$ independent observations is \n",
    "$$\\ell(\\boldsymbol{\\beta}|\\mathbf{y}) = \\sum_{i=1}^n \\log p(y_i|\\beta,\\mathbf{x}_i). $$\n",
    "The maximum likelihood estimator is defined as\n",
    "\n",
    "\\begin{align}\n",
    "\\widehat{\\boldsymbol{\\beta}} = \\underset{\\beta}{\\operatorname{argmin}} -\\ell(\\boldsymbol{\\beta}|\\mathbf{y}),\\label{neglikelihood} \\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "and it can be shown that an approximate variance-covariance (useful for confidence intervals) matrix can be obtained as\n",
    "\n",
    "$$\\mathrm{Cov}(\\widehat{\\boldsymbol{\\beta}}) = -\\left( \\nabla \\nabla^\\top \\ell(\\widehat{\\boldsymbol{\\beta}}|\\mathbf{y}) \\right)^{-1},$$\n",
    "\n",
    "where $\\nabla \\nabla^\\top \\ell(\\widehat{\\boldsymbol{\\beta}}|\\mathbf{y})$ denotes the Hessian matrix evaluated at the likelihood mode $\\widehat{\\boldsymbol{\\beta}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimisation in (1) can be carried out using built-in optimisers in `scipy` or by designing our own optimiser. We will start by demonstrating the first approach using the `minimize` function from the `scipy.optimize` module. This function also returns the inverse of the Hessian evaluated at the mode, which is useful for constructing approximate confidence intervals. We first need to simulate the data and split into train and test (as per the previous week) and, moreover, code the log-likelihood function of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEWCAYAAABi5jCmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5U0lEQVR4nO3deZgU5bnw/+/d3dOzscsSZBEMiwICQQRc2FUQUZAYw+JC9MQYY8yb5Jej58R4krxJNCdG4/saj5dv3AAFxaigAVcEQUUFRRFZRZABlHWA2bp7uu/fH9UzdjfdM90zPd2z3J/rmmu6q5566q6q7rq7qp6qR1QVY4wxpoor2wEYY4xpXCwxGGOMiWKJwRhjTBRLDMYYY6JYYjDGGBPFEoMxxpgolhhMWonIQyLymzpM11NESkTE3RBxNSYislxErsvg/EaLyNYGqvu3IrKgHtNvEpFx6YvIpIPYfQwtl4jsAv5NVV9vSfM26SMivwX6qOrVSZR9HChS1TsaOi5TP3bEYJoVEfGkub5mfwRjTCxLDOYkIpIrIn8TkX3hv7+JSG7E+H8Xkf3hcf8mIioifcLjHheRP4RfdxSRl0SkWESOiMhqEXGJyHygJ/Bi+PTRv4tIr3A9nvC0HUTksfA8jorICwlinSsib4vIfSJyBPhtOP57RORLEfk6fHorP4X4/0dElolIKTBeRE4VkX+KyEER+UJEbo2oa4SIrBOR4+F53RseniciC0TkcHj5PxCRLuFxK0Xk38KvXSJyh4jsFpEDIjJPRNqGx1Wtk+vCy3JIRH5dw3abIiKficgJEdkrIv9fePg4ESmKKLdLRH4lIp+ISKmIPCIiXcKnuE6IyOsi0j7etBHTX5gghsUi8pWIHBORt0RkYHj4jcAc4N/D2/zF2Lpq+txVxSEivwyvp/0i8oNE68LUjyUGE8+vgVHAUGAIMAK4A0BEJgO/AC4E+gBja6jnl0AR0AnoAvwnoKp6DfAlcJmqtlLV/44z7XygABgIdAbuq2E+I4Gd4XJ/BP4M9AvH3wfoBtyZQvyzw/W0Bt4BXgQ+DtczEfhfIjIpXPZ+4H5VbQN8G3gmPPw6oC3QAzgFuAkojzOvueG/8cDpQCvggZgyFwD9w/O+U0TOTLAeHgF+pKqtgUHAigTlAL4LXISzni4DluNsn444+4VbE09ao+VAX5xt8SHwJICqPhx+/d/hbX5ZnGkTfu7CvoWzTrsBNwB/r0pgJr0sMZh45gC/V9UDqnoQ+B1wTXjcVcBjqrpJVcvC4xIJAF2B01Q1oKqrNYmLWiLSFbgEuElVj4anXVXDJPtU9f+qaiVQAfwQ+LmqHlHVE8CfgJkpxL9EVd9W1RBwFtBJVX+vqn5V3Qn8v4j6AkAfEemoqiWqujZi+Ck459+DqrpeVY/Hmdcc4F5V3amqJcB/ADMl+pTY71S1XFU/xklQQxKshwAwQETahNfbhzWss/+rql+r6l5gNfCeqn6kqj7geeA7NUybkKo+qqonwvX8FhhSdQSUhJo+d+As3+/Dn4dlQAlOwjRpZonBxHMqsDvi/e7wsKpxeyLGRb6O9RdgB/CqiOwUkduTnH8P4IiqHk2yfGQMnXCONNaHT+EUAy+Hh0Ny8UcOOw04taqucH3/iXMEBM4v137AlvDpoqnh4fOBV4BF4dMi/y0iOXHmFW9deyLqB/gq4nUZzlFFPN8FpgC7RWSViJyboBzA1xGvy+O8TzSPhETELSJ3i8jnInIc2BUe1THJKmr63AEcDif/KjWtC1MPlhhMPPtwdohVeoaHAewHukeM65GokvAvx1+q6uk4pyt+ISITq0bXMP89QAcRaZdkvJF1HcLZsQ1U1Xbhv7aqWrUDSSb+yPr2AF9E1NVOVVur6pTwMm5X1Vk4p07+DDwrIoXhX7W/U9UBwHnAVODaOPOKt64rid5RJ0VVP1DVaeFYXuCb01r1UYqTaIHqi/GdEpSdDUzDOU3XFuhVNVlViLXMq6bPnckgSwwmJ3yhtOrPAywE7hCRTiLSEef8fFVb9WeAH4jImSJSEB4Xl4hMFZE+IiLAcSAY/gNnx3d6vOlUdT/OueoHRaS9iOSIyJhkFiZ8+uf/AfeJSOdwHN0irgkkHX/Y+8BxEblNRPLDv4oHicg54bqvFpFO4fkWh6cJish4ETkrvCM9jnMaJBin/oXAz0Wkt4i0wjnt9XTML+NaiYhXROaISFtVDfDN+q6vbUCeiFwaPuK5A8hNULY14AMO4ySTP8WMT7jNw2r63JkMssRgluH8wq76+y3wB2Ad8AmwEeci4h8AVHU58H+AN3FOE70brscXp+6+wOs454LfBR5U1ZXhcXfh7ASKq1rPxLgGZ2e6BTgA/K8Ulum2cGxrw6c0Xid8LjrF+FHVIM7RzlDgC5wjkn/g/CIGmAxsEpESnAvRM1W1AudC6bM4O+jNwCri7+QexTnt9Fa4/grgpyksa6RrgF3hZb4JqPXegtqo6jHgZpxl3otzBFGUoPg8nNM/e4HPgLUx4x/BuQZSLPFbmSX83JnMshvcTL2EW8h8CuSm+iu3MWjq8RvTEOyIwaRMRK4In7poj3Ne/cWmtFNt6vEb09AsMZi6+BFwEPgc5zz2j7MbTsqaevzGNCg7lWSMMSaKHTEYY4yJktYHjmVDx44dtVevXtkOwxhjmpT169cfUtW496Q0+cTQq1cv1q1bl+0wjDGmSRGR3YnG2akkY4wxUSwxGGOMiWKJwRhjTBRLDMYYY6JYYjDGGBMlY62SRORRnEcPH1DVQXHGC85DyKbgPGd9bi0djZhmau02H8+vLedISYjCXECE0gqlQysXV4zKZ1S/RA/3rL2+utZRl3pjx591moeNuytPKp9MuR37A6z+zE9IwSUweoCXPl1zap3/ojVllFY4N7EW5sLwPt64McSL2SUQUmpdZ/HmM3N04Un1zl9Zij/84BEBxgysfRnqsw0ix3ndEAg6z/2uWn9Xj43flcOCVSVJr+tkt3Ei8eYVG9fabT4WrS6lNPyYR7cLQqHklqWuMnbnc/ixySXAvASJYQrOUyWn4HTVeL+qjqyt3uHDh6s1V20+YncgsbweuGZcYdI7j3j1pVpHXeqtbTmqyp/b38u7W/01lqvaQccSojs4iJ3/4ytKCYZqXo5kY060zhLNxy0wd+I39T76emnczhhily2VbVPTNgBqXf9jB568Q12wqoRVm/wnlY23rpPZdjUtT6J5Rca1dpuPx98oJVjLbjrestRGRNar6vB44zJ2KklV3wKO1FBkGk7S0HD3iO3CXTyaFuT5teU1ftH8lU6Z+tSXah11qbe25agqv/qzmncsED8pwMm93sTOv7akkErMidZZovkENbreRPu12GVLZdvUtA2SWf+rPzt5pxxvGMRf18lsu5qWJ9G8Ioc/v7a81qRQU1111ZiuMXQjukvFovCwk4jIjSKyTkTWHTx4MCPBmcw4UlL73iyZMrWVTaWOutSbbP2Jdvp1ler8U5km3viapqlLLKmUr2kbJFNHvHWfyvZItmyiWBJNHzk8W5+jxpQYJM6wuIurqg+r6nBVHd6pU6JeBk1T1KFV7R/JZMrUVjaVOupSb7L1u+J96ush1fmnMk288TVNU5dYUilf0zZIpo546z6V7ZFs2USxJJo+cni2PkeNKTEUEd3/bnesv9cW54pR+XhraBLh9Thl6lNfqnXUpd7alqOq/OgB3lrLJfrSxw6Onb87iW93sjEnWmeJ5uOW6HoT7bdily2VbVPTNkhm/Y8e4E1qGMRf18lsu5qWJ9G8IodfMSofdxI7/UR11VVjelbSUuAWEVmEc/H5WLjvX9OCVF2kS1erpNj60tUqqbZ6441P1GKlT9f0t0qq+p9Kq6TYmJNplZRoPpGtkqr+p7tVUjLbNtVWSc6w5FslJbPtEi1PonlFxlW9fptxq6SFwDigI06n4P8F5ACo6kPh5qoP4PShWwb8QFVrbW5krZKMMSZ1NbVKytgRg6rOqmW8Aj/JUDjGGGMSaEzXGIwxxjQClhiMMcZEscRgjDEmiiUGY4wxUSwxGGOMiWKJwRhjTBRLDMYYY6JYYjDGGBPFEoMxxpgolhiMMcZEscRgjDEmiiUGY4wxUSwxGGOMiWKJwRhjTBRLDMYYY6JYYjDGGBPFEoMxxjRBwa+/JvjVVw1Sd2Pq89kYY0wNVJXgl1/ie/ttKrdvx92rF62uuy7t87HEYIwxjZyqUrltG741awgWFTkDPR7cnTqhoRDiSu/JH0sMxhjTSGkwSGDjRnzvvEPo4EEAJC8P74gReEeMwFVY2CDztcRgjDGNjIZCTkJYtYrQ0aMASJs25J57Lt5hwxCvt0Hnb4nBGGMaCVUl8Nln+FauJHToEACuDh3IHT2anLPOQtzujMRhicEYY7JMVancupWKlSsJff01ANKuHXljx5IzeHDaryHUxhKDMcZkUWVRERWvvkpwzx7AOWWUN2YMOUOHZuwIIZYlBmOMyYJQcTEVb7xB4NNPAZDCQnJHj8Z79tmIJ7u7ZksMxhiTQerz4Vu9Gt/atRAMgttN7rnnknvBBUhubrbDAywxGGNMRqgqgQ0bqHj9dbSsDICcs84ib8IEXO3aZTe4GJYYjDGmgQW//pryf/2r+jqCu0cP8i6+GE/37lmOLD5LDMYY00DU76di5Ur8a9eCKlJYSN7FFztNT0WyHV5ClhiMMSbNVJXKLVsof/ll9PhxALznnEPehAlIXl6Wo6udJQZjjEmj0IkTlP/rX1Ru3QqAu2tX8i69FE+3blmOLHkZTQwiMhm4H3AD/1DVu2PGtwUWAD3Dsd2jqo9lMkZjjKkLVSWwcSMVy5ejFRWQm0vehAl4hw/P+A1q9ZWxxCAibuDvwEVAEfCBiCxV1c8iiv0E+ExVLxORTsBWEXlSVf2ZitMYY1IVe5Tg6dOH/Msuw9WmTZYjq5tMHjGMAHao6k4AEVkETAMiE4MCrcW5KtMKOAJUZjBGY4xJmqoS+OQTyl9+GcJHCfmTJjl3LTfii8u1yWRi6AbsiXhfBIyMKfMAsBTYB7QGvq+qodiKRORG4EaAnj17NkiwxhhTk1BZGeVLl35zlNC3L/lTpzbZo4RImUwM8dKnxryfBGwAJgDfBl4TkdWqejxqItWHgYcBhg8fHluHMcY0qMqdOyl7/nm0pMQ5Spg8mZwhQ5r0UUKkTCaGIqBHxPvuOEcGkX4A3K2qCuwQkS+AM4D3MxOiMcYkpsEgFStW4H/nHQDcPXtScMUVje7O5frKZGL4AOgrIr2BvcBMYHZMmS+BicBqEekC9Ad2ZjBGY4yJK3joEGXPPUdo/34QIXfsWHJHj25yLY6SkbHEoKqVInIL8ApOc9VHVXWTiNwUHv8Q8L+Bx0VkI86pp9tU9VCmYjTGmHj8H31E+fLlEAgg7dpRMGMGnh49ap+wicrofQyqugxYFjPsoYjX+4CLMxmTMcYkooEA5cuWEdiwAXAeepc/ZUqTuHu5PuzOZ2OMiSN45Ahlzzzj9Kjm8ZB/6aV4hw7NdlgZYYnBGGNiBLZsoeyFF8Dnw9WhAwVXXYW7S5dsh5UxlhiMMSZMQyEq3nijutWR54wzKJg2rdmfOoplicEYY3BuWCtbvJjgrl0gQt6FF+I999xmc29CKiwxGGNavODXX1O6aBFaXIy0akXBlVfiOe20bIeVNZYYjDEtWmDLFsqeew4CAdynnkrBzJm4WrfOdlhZZYnBGNMiqSq+1avxvfkmEG6KetllSE5OliPLPksMxpgWRwMBypcsIbBpE4BzPeG881rk9YR4LDEYY1qU0IkTlC5c6Dzawuul4LvfJadfv2yH1ahYYjDGtBjBAwcoffJJ9PhxXO3bUzBzJu7OnbMdVqNjicEY0yJU7txJ6TPPgM+Hu0cP5yJzQUG2w2qULDEYY5o9/4YNlL/4IoRCeAYMoGD6dLvIXANLDMaYZktV8a1ahW/VKgC8555L3kUX2UXmWlhiMMY0SxoMUv7iiwQ+/ti5k/mSS8g955xsh9UkWGIwxjQ76vdTtngxlTt2QE6O0/Kof/9sh9VkWGIwxjQrofJyyp56imBREVJQQMHs2Xi6dct2WE2KJQZjTLMROn6c0gULCB08iLRtS+HVV+Pu2DHbYTU5lhiMMc1C8PBhSufPR48dw9WpE4VXX42rTZtsh9UkWWIwxjR5wX37nBvXyspwd+9OwaxZdo9CPVhiMMY0aZW7dlG6cCH4/Xj69KHge99DvN5sh9WkWWIwxjRZgR07KHv6aaisJGfQIPKnT0fc7myH1eRZYjDGNEmBLVsoe/ZZCAbJGTaM/KlT7ca1NLHEYIxpcvyffkr5889DKIR3xAjyJk+2pJBGlhiMMU2Kf8MGypcuBVVyzz+f3IkTLSmkmSUGY0yT4Vu3jop//QuA3HHjyB0zxpJCA7DEYIxpEnxr11LxyiuA0+Na7vnnZzmi5ssSgzGm0atYvRrfihUAzsPwRozIckTNmyUGY0yjVvHWW/jefBOA/Msvx/ud72Q5oubPEoMxptGqWL36m6QwfTreIUOyHFHL4Ep1AhEpFBG7g8QY06Aq1qypPn1kSSGzak0MIuISkdki8i8ROQBsAfaLyCYR+YuI9E12ZiIyWUS2isgOEbk9QZlxIrIhXP+q5BfFGNNc+N5+G98bbwCQP22aJYUMS+aI4U3g28B/AN9S1R6q2hkYDawF7haRq2urJHyU8XfgEmAAMEtEBsSUaQc8CFyuqgOB76WwLMaYZsD3zjtUvP46EE4KQ4dmN6AWKJlrDBeqaiB2oKoeAf4J/FNEkulVewSwQ1V3AojIImAa8FlEmdnAc6r6ZXgeB5Ko1xjTTPjefZeK114DwheaLSlkRa1HDFVJQUT+JgnuJImXOOLoBuyJeF8UHhapH9BeRFaKyHoRuTZeRSJyo4isE5F1Bw8eTGLWxpjGzvfuu1S8+ioA+ZddZq2PsiiVi88lwFIRKQQQkYtF5O0Upo+XVDTmvQc4G7gUmAT8RkT6nTSR6sOqOlxVh3fq1CmFEIwxjZFv7dpvksLUqXiHDctyRC1b0s1VVfUOEZkNrBQRH1AKxL2AnEAR0CPifXdgX5wyh1S1FCgVkbeAIcC2FOZDIBCgqKiIioqKVCYzLUxeXh7du3cnJyeZM6Gmofjee6/6jub8qVPxnn12liMySScGEZkI/BAnIXQFblDVrSnM6wOgr4j0BvYCM3GuKURaAjwgIh7AC4wE7kthHgAUFRXRunVrevXqZc9RMXGpKocPH6aoqIjevXtnO5wWy7duHRUvvwxA3qWXWlJoJFI5lfRr4E5VHQdcCTwtIhOSnVhVK4FbgFeAzcAzqrpJRG4SkZvCZTYDLwOfAO8D/1DVT1OIEYCKigpOOeUUSwomIRHhlFNOsaPKLPJ//HH1A/HyLrmE3OHDsxyRqZLKqaQJEa83isglOK2SzkuhjmXAsphhD8W8/wvwl2TrTMSSgqmNfUayJ/DZZ5QvWQKEH4hnzz5qVGpNDCLSs4bRN0SML1bV4+kJyxjTXAW2baPsn/90+lMYM8aektoIJXPE8ARO66FErYok/P9xYF7aIjPGNDuVX3xB2TPPOD2vnXsuuePGZTskE0cy9zGMV9UJ4f+xfxMi/ltSiOF2uxk6dCiDBg3ie9/7HmVlZXHLlZeXM3bsWILBYFrnf/3119O5c2cGDRoUNXzcuHHs2rUr4XR+v58xY8ZQWVmZ1nhMy1a5Zw+lCxdCMIj37LPJu+giO53XSKX8EL1IIvKDdAXSHOXn57NhwwY+/fRTvF4vDz0UdTkFVSUUCvHoo48yY8YM3O70Pptw7ty5vBxu8ZEKr9fLxIkTefrpp9Maj2m5gvv3U/rkkxAIkDNkCHmXXmpJoRGrV2IAfpeWKFqA0aNHs2PHDnbt2sWZZ57JzTffzLBhw9izZw9PPvkk06ZNA2Djxo2cH3HO9cMPP2TChKQbf0UZM2YMHTp0qLHM+PHjeS38CII77riDW2+9FYDp06fz5JNP1mm+xkQKHjhA6fz54PORM2AA+ZdfbkmhkUvm4vMniUYBXdIbTvod+13D5K62//VfSZetrKxk+fLlTJ48GYCtW7fy2GOP8eCDD+L3+9m5cye9evUCYODAgXz++ecEg0Hcbje//OUv+etf/1pd1+jRozlx4sRJ87jnnnu48MILU16O3/3ud9x5550cOHCAjz76iKVLlwIwaNAgPvjgg5TrMyZS8PBhSufNQ8vL8fTtS/6MGYirvr9HTUNL5uJzF5zHUxyNGS7AO2mPqBkpLy9naPghYKNHj+aGG25g3759nHbaaYwaNQqAQ4cO0a5du+ppXC4XAwcOZNOmTWzfvp2ePXsyLOLxAKtXr05rjGPGjEFVuffee1m5cmX16Sy3243X6+XEiRO0bt06rfM0LUOouNhJCqWluHv3puCqq5A0ny41DSOZxPAS0EpVN8SOEJGV6Q4o3VL5ZZ9uVdcYYhUWFkaVib3JatSoUbz99ts8+OCDJ10jSPcRw8aNG9m/fz8dO3Y8KQH4fD7y8vJSrtOY0IkTTlI4fhx3jx4UzpyJeKzDyKai1i2lqjfUMC72kRYmRe3btycYDFJRUVG9Ex41ahRz587lJz/5Cd26RT+ANp1HDPv372fOnDksWbKEW2+9lVdeeYVJkyYBcPjwYTp16mTPETIpC5WWUjp/PqGjR3F17Urh7NmI15vtsEwK7GRfI3DxxRezZs2a6vdnnHEGubm53HbbbfWqd9asWZx77rls3bqV7t2788gjj1SPKysrY8aMGfz1r3/lzDPP5De/+Q2//e1vq8e/+eabTJkypV7zNy2PVlRQumABoYMHcXXqROHVVyN21Nnk2LFdAyopKTlpWK9evfj00+jHP91yyy3ce++91aeC7r//fu66666oU051sXDhwoTjCgoKePfdd6vfjxkzJur9U089xV133VWv+ZuWRX0+Sp98ktBXX+Hq0IHCa6/FVVCQ7bBMHdgRQyPwne98h/Hjx/P5559zxhlnUF5eznXXXZe1ePx+P9OnT6d///5Zi8E0LRoIULpoEcGiIqRtWycptGqV7bBMHaV0xCAiE1R1RdX/hgqqJbr++usB2LJlS4PPa+7cuVEtoWJ5vV6uvTZu53nGnEQrKyl75hmCu3YhrVo5SaFt22yHZeoh1SOGe2L+myaotsRgTLI0FKLsueeo3LEDKSig8NprcddyU6Vp/Op6KsluWzSmhVNVypcsoXLzZsjLo/Dqq3FbV7vNgl1jMMakTFWpeOklAp98Al4vhXPm4O7aNdthmTSxxGCMSYmqUvHKK/g//BA8HgpnzcLTvXu2wzJpZInBGJMS35tv4n/vPXC5KLjqKjzh53yZ5iPVxFDVMP/kZzIYY5q9ijVr8K1eDSIUXHklOX37Zjsk0wBSSgyqOibyvzGm5fC9/z6+N94AIH/6dHLOPDPLEZmGYqeSGkhV720DBw5kyJAh3HvvvYRCoRqn2bVrF0899VTK86pPD3Bz587l2WefTXm6WH/6059Snmbx4sWceeaZjB8//qRxkydPpl27dkydOjVq+BdffMHIkSPp27cv3//+9/H7/YBz3vvWW2+lT58+DB48mA8//BCw3ujSxf/RR1QsXw5A/tSpeAcPznJEpiElnRhE5HURGdKQwWTL2m0+bptXzA8fPMJt84pZu81X7zqrnqy6adMmXnvtNZYtW8bvaukboq6JoaF6gEtFXRLDI488woMPPsibb7550rhf/epXzJ8//6Tht912Gz//+c/Zvn077du3r37+0/Lly9m+fTvbt2/n4Ycf5sc//jFgvdGlg//TTykP99ORN2kS3rPPznJEpqGlcsTw78B9IvKYiDSbdmlrt/mYv7KUIyXOr/kjJSHmryxNS3Ko0rlzZx5++GEeeOABVJVdu3YxevRohg0bxrBhw3jnHadbi9tvv53Vq1czdOhQ7rvvvoTlYkX2ALd//37GjBlT3dd01dNYW0U8nuDZZ59l7ty51e9ff/11Ro8eTb9+/XjppZcA2LRpEyNGjGDo0KEMHjyY7du3A7BgwYLq4T/60Y8IBoPcfvvt1X1PzJkz56T4Fi5cyFlnncWgQYOqHwz4+9//njVr1nDTTTfxq1/96qRpJk6ceNJjwFWVFStWcOWVVwJw3XXX8cILLwCwZMkSrr32WkSEUaNGUVxczP79+wHrja4+Alu3Uv788wDkjh9PbrgfEdO8Jf1IDFX9EJggIt8FXhaR54D/VtXyBosuA55fW44/5iyDv9IZPqpfbtrmc/rppxMKhThw4ACdO3fmtddeIy8vj+3btzNr1izWrVvH3XffzT333FO9cy4rK4tbLirWmB7gnnrqKSZNmsSvf/1rgsEgZWVltca2a9cuVq1axeeff8748ePZsWMHDz30ED/72c+YM2cOfr+fYDDI5s2befrpp3n77bfJycnh5ptv5sknn+Tuu+/mgQceiNv3xL59+7jttttYv3497du35+KLL+aFF17gzjvvZMWKFdxzzz0MHz48qXV4+PBh2rVrhyf8XP/u3buzd+9eAPbu3UuPHj2qy1aN69q1q/VGV0eBzz+nbPFiCIXIPf98ckePznZIJkNSfVaSAFuB/wH+APxQRP5DVU8+5m8iqo4Ukh1eH6oKQCAQ4JZbbmHDhg243W62bdsWt3wy5WJ7gDvnnHO4/vrrCQQCTJ8+vboHuZpcddVVuFwu+vbty+mnn86WLVs499xz+eMf/0hRUREzZsygb9++vPHGG6xfv55zzjkHcK5tdO7cuca6P/jgA8aNG0en8B2xc+bM4a233mL69Om1xhWrav1Fquo7uKZx1htd6ip376Zs0SIIBvGecw65EydaP80tSCrXGNYAe4H7gG7AXGAcMEJEHm6I4DKhQ6v4qyDR8LrauXMnbrebzp07c99999GlSxc+/vhj1q1bV30BNVYy5WJ7gBszZgxvvfUW3bp145prrmHevHkAUV/q2B7jYr/wIsLs2bNZunQp+fn5TJo0iRUrVqCqXHfddWzYsIENGzawdevWqD4c4om3w66rjh07UlxcXH0huaioiFNPPRVwjhD27NlTXTZyHFhvdKmo3LuX0qeegspKcoYOJe+SSywptDCp7P1uArqp6kWq+htVfUlVd6jqT4Eme4x5xah8vDHHTV6PMzxdDh48yE033cQtt9yCiHDs2DG6du2Ky+Vi/vz51a2JWrduHdVtZ6JykSJ7gAPYvXs3nTt35oc//CE33HBDdeucLl26sHnzZkKhEM+HzxlXWbx4MaFQiM8//5ydO3fSv39/du7cyemnn86tt97K5ZdfzieffMLEiRN59tlnOXDgAABHjhxh9+7dAOTk5BAIBE6Kb+TIkaxatYpDhw4RDAZZuHAhY8eOrdN6FBHGjx9f3YrqiSeeqL62cvnllzNv3jxUlbVr19K2bVu6hh/RYL3RJS/49deULVgAfj85AweSf9lllhRaoFpPJYmIqOPTGopdmsaYMqrqOsLza8s5UhKiQysXV4zKr/f1haqLsYFAAI/HwzXXXMMvfvELAG6++Wa++93vsnjxYsaPH1/dIc/gwYPxeDwMGTKEuXPnJiwXq6oHuAsvvJCVK1fyl7/8hZycHFq1alV9xHD33XczdepUevTowaBBg6I6Eerfvz9jx47l66+/5qGHHiIvL4+nn36aBQsWkJOTw7e+9S3uvPNOOnTowB/+8AcuvvhiQqEQOTk5/P3vf+e0007jxhtvZPDgwQwbNizqQm/Xrl256667GD9+PKrKlClTqnfmNRk9ejRbtmyhpKSkuve5SZMm8ec//5mZM2dyxx138J3vfIcbbnB6np0yZQrLli2jT58+FBQU8Nhjj1XXZb3RJSd46BCl8+ejFRV4+vUj/4orEJe1aG+JpLZDfRFZCfwTWKKqX0YM9wIXANcBb6rq4w0XZmLDhw/X2Auymzdv5swWdPPNRx99xL333hu3eaeBGTNmcNddd8XteKilfVYSCR4+TOnjj6MlJXhOP52CWbMQj3Xw2JyJyHpVjdvyI5ktPxm4HlgoIr2BYiAPcAOvAvep6ob0hGrqoqoHuGAwmNV7GRoj642udsEjRyh94gm0pAR3r14UzJxpSaGFq3Xrq2qFiPwP8BKwH+gIlKtqcQPHZlJQ1QOciWa90dUsdPSokxROnMDdsyeFs2Yhdi2mxUvqBKI655ueV9WAqu6va1IQkckislVEdojI7TWUO0dEgiJyZV3mE465rpOaFqKlf0ZCxcWUPPEEevw47h49KJw9G/F6sx2WaQRSubK0VkTOqeuMRMQN/B24BBgAzBKRAQnK/Rl4pa7zysvL4/Dhwy3+i28SU1UOHz7cYpuwho4dc5LCsWO4u3WjcM4cJDd9N3Sapi2VE4njgZtEZBdQitO9p6pqsk/TGgHsUNWdACKyCJgGfBZT7qc4F7vrnIS6d+9OUVERBw8erGsVpgXIy8ujewvsYCZ0/Lhz+qi4GPepp1J49dWWFEyUVBLDJfWcVzdgT8T7ImBkZAER6QZcAUyghsQgIjcCNwL07NnzpPE5OTn07t27nuEa0/yETpyg9IknCB09iqtrVycptNCjJpNYMvcx5OHc3NYH2Ag8oqp1eYZxvLtkYs/1/A24TVWDNd1Uo6oPAw+D01y1DrEY0+KESkoonTeP0JEjuL71LQqvuQbJT9+NnKb5SOaI4QkgAKzmm+sDP6vDvIqAHhHvuwP7YsoMBxaFk0JHYIqIVKrqC3WYnzEmLFRa6iSFQ4dwde5M4TXX4LKkYBJIJjEMUNWzAETkEeD9Os7rA6Bv+F6IvcBMYHZkAVWtPv8jIo8DL1lSMKZ+QidOfJMUOnWi8NprcRUUZDss04glkxiqH4CjqpV1fW5KeNpbcFobuYFHVXWTiNwUHv9QnSo2xiRUdaE5dOTIN0khwaNVjKmSTGIYIiLHw68FyA+/r2qV1CbZmanqMmBZzLC4CUFV5yZbrzHmZKHiYudI4ehRXF26OKePLCmYJCRz57M9Y8GYJiZ09Og39yl07UqBXVMwKbAHohjTzAQPH6Z03jznjubu3Z2b16xJqkmBJQZjmpHgwYNOUigpcZ59NHu23bxmUmaJwZhmInjggJMUSktx9+rlPBDPnn1k6sASgzHNQHDfPkoXLEDLy53+FGbOtKekmjqzxGBME1e5axelCxeC34+nb18KrrrK+lMw9WKfHmOasMDWrZQtXgzBIDmDBpE/fTpinTWZerLEYEwT5f/kE8pfeAFU8Z59NnlTplgfzSYtLDEY0wT53n+fiuXLAci94AJyJ0ygrk8lMCaWJQZjmhBVxbd6Nb433wQg78ILyT3//CxHZZobSwzGNBEaClHx8sv4P/gAgPypU/GefXaWozLNkSUGY5oADQQoe+45KrdsAbebgiuuIGfgwGyHZZopSwzGNHKhsjLKFi0iuGcP5OVR+P3v4+nVK9thmWbMEoMxjViouJjSBQsIHT6MtGlD4Zw5uDt3znZYppmzxGBMIxXcv5/Sp55CS0qcXtfmzMHVJumn3BtTZ5YYjGmEAtu3U/bss+D3O889+v737QmpJmMsMRjTiKgq/vfeo+LVV0HVuZt52jR7xIXJKPu0GdNIaDBIxfLl+NevByB37Fhyx461G9dMxlliMKYR0PJyShcvJvjFF+B2kz99Ot5Bg7IdlmmhLDEYk2XBw4cpW7jQaXlUWEjBzJl4unfPdlimBbPEYEwWVe7cSdnixWhFBa4uXSicNQtX27bZDsu0cJYYjMkCVcX/zjtUvPEGqOLp14+CGTOsG07TKFhiMCbD1OejbMkSKjdvBiB3zBjnIrM9Mts0EpYYjMmg4KFDlD39NKFDhyA313nmUf/+2Q7LmCiWGIzJkMCWLZS98AL4fLg6daLgqqtwd+yY7bCMOYklBmMamAaD+N58E9/bbwPgGTCAgssvt+sJptGyxGBMAwoVF1P2z38SLCoCEfImTsR73nl205pp1CwxGNNAAps3U7Z0KVRUIG3aUDBjBp7TTst2WMbUyhKDMWmmlZVUvPpqdU9rnn79yJ82DVdBQZYjMyY5lhiMSaPgoUOUPfssoa+/BpeLvIsuwjtypJ06Mk1KRhODiEwG7gfcwD9U9e6Y8XOA28JvS4Afq+rHmYzRmLpQVfzvv0/F669DZSWu9u0puPJK3Keemu3QjElZxhKDiLiBvwMXAUXAByKyVFU/iyj2BTBWVY+KyCXAw8DITMVoTF2EiospW7KE4K5dAOQMGUL+JZdYqyPTZGXyiGEEsENVdwKIyCJgGlCdGFT1nYjyawF7kphptFSVwIYNlL/8Mvj9SEEB+ZddRs4ZZ2Q7NGPqJZOJoRuwJ+J9ETUfDdwALI83QkRuBG4E6NmzZ7riMyZpoZISyl98kcpt2wDwnHEG+VOn4ioszHJkxtRfJhNDvKtvGregyHicxHBBvPGq+jDOaSaGDx8etw5jGoKqEvjoI8pfew0qKiA3l/xLLiFn8GC7wGyajUwmhiKgR8T77sC+2EIiMhj4B3CJqh7OUGzG1Cp46BDlL71EcPduADzf/jb5l11mj8k2zU4mE8MHQF8R6Q3sBWYCsyMLiEhP4DngGlXdlsHYjElIg0F8a9bgW70agkGkoIC8yZPJGTTIjhJMs5SxxKCqlSJyC/AKTnPVR1V1k4jcFB7/EHAncArwYPgLV6mqwzMVozGxKr/8kvKXXiJ08CAAOUOHknfRRXazmmnWRLVpn6IfPny4rlu3LtthmGYmdOwYFa+/TuDTTwFwdehA/tSpeHr3znJkxqSHiKxP9MPb7nw2JoIGAvjeecd5EmogAB4PueedR+7o0YjHvi6mZbBPujE4rY0qN2+m/NVX0WPHAMgZMMA5bdSuXXaDMybDLDGYFq+yqIiK11+vbm3k6tKF/MmT8fTqld3AjMkSSwymxQoeOEDFihVUbt0KgOTnkzthAt5hw6z/ZdOiWWIwLU6ouJiKlSsJfBx+PmNODrkjR5J7/vlIXl52gzOmEbDEYFqM0PHj+N5+G//69RAMgsuF9+yzyR0zBlerVtkOz5hGwxKDafZCR4/iW7MG/8cfOwkByBk8mLxx43C1b5/l6IxpfCwxmGYrePAgvjVrCGzcCOH7dXIGDCB3zBjcXbpkOTpjGi9LDKbZqSwqwvfuu1R+Fn6iuwg5Q4aQe8EFuDt2zG5wxjQBlhhMs6DBIIHNm/GvXUtw715noNuNd+hQcs8/304ZGZMCSwymSQuVl+Nfvx7/+++jJ04AIHl5eM8+G++IEbjatMlyhMY0PZYYTJOjqgT37MH/4YcENm2CykoAXB074h05Eu/gwYjXm+UojWm6LDGYJiNUVkbg44/xf/RR9dNOwekXwTtqFJ5vf9seg21MGlhiMI2ahkJU7txJ4OOPCWzeXN3cVAoL8Q4dSs6wYbg7dMhylMY0L5YYTKOjqgT37SPwyScENm1CS0urx3n69ME7bBiefv0QtzuLURrTfFliMI1G8OBBAps2Edi4kdCRI9XDXaecQs5ZZ+EdMsSedGpMBlhiMFmjqgT376dy82YCW7YQOnSoepy0akXOwIHkDB6Mu2tXu3ZgTAZZYjAZpcEgwd27CWzbRmDLluq+D8B5uqmnf39yBg3C07u3PeHUmCyxxGAaXOjYMQLbt1O5YweVX3wBfn/1OGndmpwzziDnzDNxn3aaJQNjGgFLDCbttKKCyt27qdy1i8rPP49qWgrg6twZT58+TjLo1s1OExnTyFhiMPWmFRVUfvmlkwh27SK0f390Aa8Xz+mnk9OnD54+fXC1bZudQI0xSbHEYFKiqmhxMZVFRQSr/vbvr356KQAuF+7u3fH06oWnd2/cPXpY01JjmhBLDKZG6vMR3LfPSQR79xIsKoq6rwCITgS9ejmJICcnOwEbY+rNEoMBwkcCJSUEv/qK4FdfEar6H3E/QRXJz8fdvbvz160bnh497NlExjQjlhhaoFB5OaGDBwkdOkTw4EFCBw8S/Oqrk48EwDka6NLlm0TQvTuu9u3tgrExzZglhmZKQyH0xAmChw4ROnTI2fmHX8dNAAC5ubi/9a2oP1enTnZ9wJgWxhJDE6Y+H6GjR+P/FRdDKBR/wpwcXB074u7Y0fnfqZNzd3HbtnYkYIyxxNBYqd9P6Phx9PhxQuG/2NdaXl5jHdKqFa4OHb5JAp064e7Y0RKAMaZGlhgySCsr0dJSQqWlaEnJN68j/kInThA6fhx8vtor9HhwtWuHq337k//atbMLwsaYOrHEUAcaCqHl5Sf/VVTEH15eTqi0NLmdfRWPB1ebNkibNrjatMHVuvU3r8PDpbDQfvkbY9KuxSaG4JEjaHEx6vOhfr/z3+eD8P+oYRGv1eeDQKBuM3W5nJ15YSGuwkKkVSukoABXq1bfDG/Vytnp5+fbTt8YkxUZTQwiMhm4H3AD/1DVu2PGS3j8FKAMmKuqHzZELP6338b/Yd2qDiGEvHl4W+U7O/D8fA4HvGw/msPRSi/k5dOxcwHbj+bwtS+XClcux6UAvyuXIEKHfBdXjMgH4Pm15RwpCdGhlYsrRuXDcVi0rIzSioqT5itAjhv8QejQysVZp3nYuLuSIyUhCvMEVCn1UV3XqH65AKzd5mPRmjJKK765O7kwT5h5QUFUmdhYRvXLPWl41DxzARFKKxSXQEhPnnds3VHL4gF/ZeJpFq0updQXP95ky0SKjMPrgUAlKOASGD3AS5+uOXHXQSLx1lm8bRpvHde0vWqyYFUJqz/zE4q40Tx2u6RSX7LLFll/Tds6to7I7ZObAx4XUctc0/pKFE/V/JOJI9HnOpnlj/zOFObCzNGFKa/Tus4/XdPXlWjkowwackYibmAbcBFQBHwAzFLVzyLKTAF+ipMYRgL3q+rImuodPny4rlu3LuV4fO+9x+EPP2PXMTcVeKkQ588nOfhcVa+/GVb13ide/JKDyyXMnVhYvfOcv7IUf2Xy83e7AIVgxOp3i7OjCqVhk3g9cM24QgAeX1FKME4DJbcL5k5wysTG7/XAuf29vLvVn9JyRc47lXUTO83jb5RGrZvIeJMtEymZOARn/ceLKVa8+tziVBK5riO3Q03zr2leVRasKmHVJn/C8anWl0gqn+dE80m0fSLVtL5ifwDUFk+y0yWzXtZu88X9zriF6u98Muo6/3RNXxsRWa+qw+ONy+QRwwhgh6ruDAe1CJgGfBZRZhowT51stVZE2olIV1Xdf3J19ZM7ciT3b+3PEU3QpLMWQXV+6Yzql8vza8tT3nnG21HX9CVKlb/SiS/RvKqGV5WJjd9fyUm/TFOddyrrJnaaeOuiKt5ky0RKJo7Y6iJjihWvvqCeXEnkdqhp/jXNq8rqz5JLCsnWl0gqn+dE80m0fSLVtL4i60smnmSnS2a9PL+2POH3M5V1Wtf5p2v6+shkYugG7Il4X4RzVFBbmW5AVGIQkRuBGwF69uxZ54AiT23UZ/r61tNQkomrpjL1OXKpy7pJZppUytQ2LJWY6lNfsmVrK5fq9kj3MqdSvj7fidhp67r+6rrtUv1spVq2vp+HTOxvMtkrSrwrqbEf9WTKoKoPq+pwVR3eqVOnOgfUoVX9Fr9q+vrW01A6tHLVGltNZVz1uPZdl3WTzDSplKltWCox1ae+ZLZDMnWmuj3SvcyplK/PdyJ22mTrSna6ZL4TdRmXbNm6Lk9dYqirTO7RioAeEe+7A/vqUCZtrhiV75zrrwO3UH3x7IpR+XhTPPZyu8LnWGPqrM/OOJLX48RV0zK6Xd+UiY3f63EuyKa6XJHzhuTXTew0sesmMt5ky0RKJo7Y6iJjSqY+t3DSuo7cDjXNv6Z5VRk9IPn7UpKpL5FUPs+J5pNo+0SqaX2lGk+y0yWzXhJ9ZyK/88mo6/zTNX19ZPJU0gdAXxHpDewFZgKzY8osBW4JX38YCRxriOsLVarO00W2PvB6nJY/pb5vWj3Eim2hUPW/tlYcsa0oYqepGhbbgqhKXVslxaszXgueeK0f+nStX6uk2HUTtSwJWiVVb5caWhwlUybetk5Xq6R42zyZVjb1aZV09dhWQMO3Skr281zTfOJtn7q2SoqNJ9lWSYm2UW3rJd5+oS6tkuo6/3RNXx8Za5UE1a2O/obTXPVRVf2jiNwEoKoPhZurPgBMxmmu+gNVrbHJUV1bJRljTEvWWFoloarLgGUxwx6KeK3ATzIZkzHGmGiN86qpMcaYrLHEYIwxJoolBmOMMVEsMRhjjImS0VZJDUFEDgK76zh5R+BQGsNpCmyZWwZb5pahPst8mqrGvUO4ySeG+hCRdYmaazVXtswtgy1zy9BQy2ynkowxxkSxxGCMMSZKS08MD2c7gCywZW4ZbJlbhgZZ5hZ9jcEYY8zJWvoRgzHGmBiWGIwxxkRpEYlBRCaLyFYR2SEit8cZLyLyf8LjPxGRYdmIM52SWOY54WX9RETeEZEh2YgznWpb5ohy54hIUESuzGR8DSGZZRaRcSKyQUQ2iciqTMeYbkl8ttuKyIsi8nF4mX+QjTjTRUQeFZEDIvJpgvHp33+parP+w3nE9+fA6YAX+BgYEFNmCrAcp5uAUcB72Y47A8t8HtA+/PqSlrDMEeVW4Dzl98psx52B7dwOp1/1nuH3nbMddwaW+T+BP4dfdwKOAN5sx16PZR4DDAM+TTA+7fuvlnDEMALYoao7VdUPLAKmxZSZBsxTx1qgnYh0zXSgaVTrMqvqO6p6NPx2LU5veU1ZMtsZ4KfAP4EDmQyugSSzzLOB51T1SwBVberLncwyK9A63L9LK5zEUJnZMNNHVd/CWYZE0r7/agmJoRuwJ+J9UXhYqmWaklSX5wacXxxNWa3LLCLdgCuAh2gektnO/YD2IrJSRNaLyLUZi65hJLPMDwBn4nQLvBH4maqGaL7Svv/KaEc9WRKv59nYNrrJlGlKkl4eERmPkxguaNCIGl4yy/w34DZVDTo/Jpu8ZJbZA5wNTATygXdFZK2qbmvo4BpIMss8CdgATAC+DbwmIqtV9XgDx5Ytad9/tYTEUAT0iHjfHeeXRKplmpKklkdEBgP/AC5R1cMZiq2hJLPMw4FF4aTQEZgiIpWq+kJGIky/ZD/bh1S1FCgVkbeAIUBTTQzJLPMPgLvVOQG/Q0S+AM4A3s9MiBmX9v1XSziV9AHQV0R6i4gXmAksjSmzFLg2fHV/FHBMVfdnOtA0qnWZRaQn8BxwTRP+9Rip1mVW1d6q2ktVewHPAjc34aQAyX22lwCjRcQjIgXASGBzhuNMp2SW+UucIyREpAvQH9iZ0SgzK+37r2Z/xKCqlSJyC/AKTouGR1V1k4jcFB7/EE4LlSnADqAM5xdHk5XkMt8JnAI8GP4FXalN+MmUSS5zs5LMMqvqZhF5GfgECAH/UNW4zR6bgiS38/8GHheRjTinWW5T1Sb7OG4RWQiMAzqKSBHwX0AONNz+yx6JYYwxJkpLOJVkjDEmBZYYjDHGRLHEYIwxJoolBmOMMVEsMRhjjIliicEYY0wUSwzGGGOiWGIwJs3C/T18IiJ5IlIY7hNgULbjMiZZdoObMQ1ARP4A5OE8uK5IVe/KckjGJM0SgzENIPwcnw+ACuA8VQ1mOSRjkmankoxpGB1wOolpjXPkYEyTYUcMxjQAEVmK07tYb6Crqt6S5ZCMSVqzf7qqMZkW7iWtUlWfEhE38I6ITFDVFdmOzZhk2BGDMcaYKHaNwRhjTBRLDMYYY6JYYjDGGBPFEoMxxpgolhiMMcZEscRgjDEmiiUGY4wxUf5/F547Fp56Ak8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Simulate 100 observations from a logistic regression model\n",
    "np.random.seed(1234)\n",
    "x = sps.uniform.rvs(size = 1000)  # y will be generated given these x values\n",
    "beta0 = -4.5\n",
    "beta1 = 6.5\n",
    "z = beta0 + beta1*x \n",
    "y = sps.bernoulli.rvs(size = 1000, p = 1/(1 + np.exp(-z)))\n",
    "\n",
    "# Plot a subset of the regression points (y is zero or one)\n",
    "subset = np.random.choice(np.arange(1000), 100, replace = False)\n",
    "plt.scatter(x[subset], y[subset], color = \"cornflowerblue\", label = \"Data (subset of 100)\")\n",
    "\n",
    "# Plot the regression line (Pr(y = 1|x)) using a grid of x-values\n",
    "x_grid = np.linspace(0, 1, 200)\n",
    "z_grid =  beta0 + beta1*x_grid \n",
    "\n",
    "plt.plot(x_grid, 1/(1 + np.exp(-z_grid)), color = \"lightcoral\", linewidth=2, label = r'$\\Pr(y = 1|x)$')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(r'$\\Pr(y = 1|x)$')\n",
    "plt.title(\"Logistic regression simulation\")\n",
    "plt.legend()\n",
    "\n",
    "# Split into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_logistic(beta, y, X):\n",
    "    \"\"\"\n",
    "    Log-likelihood for the logistic model.\n",
    "    \"\"\"\n",
    "    z = np.dot(X, beta)\n",
    "    log_Pr_y_1 = -np.log1p(np.exp(-z[y == 1])) # log Pr(y = 1)\n",
    "    log_Pr_y_0 = -z[y == 0] - np.log1p(np.exp(-z[y == 0])) # log Pr(y = 0)\n",
    "    \n",
    "    return np.sum(log_Pr_y_1) + np.sum(log_Pr_y_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code estimates the parameters and performs prediction using the logistic regression model without the `LogisticRegression()` function. The code compares the results to those obtained using the `LogisticRegression()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 316.011108\n",
      "         Iterations: 12\n",
      "         Function evaluations: 39\n",
      "         Gradient evaluations: 13\n",
      "      fun: 316.01110833735197\n",
      " hess_inv: array([[ 0.12006178, -0.16896971],\n",
      "       [-0.16896971,  0.25846017]])\n",
      "      jac: array([-7.62939453e-06, -7.62939453e-06])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 39\n",
      "      nit: 12\n",
      "     njev: 13\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([-4.96727888,  7.19560121])\n",
      "Estimates      : [-4.96727888  7.19560121].\n",
      "Standard errors: [0.346 0.508]. \n",
      "Estimated intercept\n",
      "LogisticRegression(): -4.967. Our : -4.967. True value: -4.500\n",
      "Estimated slope\n",
      "LogisticRegression(): 7.195. Our : 7.196. True value: 6.500\n",
      "Predicting first 10 test observations\n",
      "LogisticRegression()\n",
      "[[0.49370842 0.50629158]\n",
      " [0.97654598 0.02345402]\n",
      " [0.09728647 0.90271353]\n",
      " [0.10596371 0.89403629]\n",
      " [0.48021172 0.51978828]\n",
      " [0.97508617 0.02491383]\n",
      " [0.98382906 0.01617094]\n",
      " [0.19546666 0.80453334]\n",
      " [0.21145111 0.78854889]\n",
      " [0.19294399 0.80705601]]\n",
      "Without a package\n",
      "[[0.49365909 0.50634091]\n",
      " [0.97654609 0.02345391]\n",
      " [0.09725872 0.90274128]\n",
      " [0.10593426 0.89406574]\n",
      " [0.48016174 0.51983826]\n",
      " [0.97508621 0.02491379]\n",
      " [0.98382946 0.01617054]\n",
      " [0.19542386 0.80457614]\n",
      " [0.21140661 0.78859339]\n",
      " [0.19290147 0.80709853]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train = np.vstack((np.ones(800), x_train)).T\n",
    "\n",
    "# Estimating the model ourselves (without LogisticRgression())\n",
    "neg_log_likelihood = lambda beta: -log_likelihood_logistic(beta, y_train, X_train)\n",
    "\n",
    "beta_optim_start = np.zeros(2)\n",
    "res_optim_likelihood = minimize(neg_log_likelihood, beta_optim_start, method='BFGS', jac = None, \n",
    "                                options={'gtol': 1e-04, 'maxiter': 1000, 'disp': True})\n",
    "\n",
    "print(res_optim_likelihood)\n",
    "# Standard errors for approximate confidence intervals.\n",
    "standard_errors = np.sqrt(np.diag(res_optim_likelihood.hess_inv))\n",
    "print('Estimates      : %s.' % res_optim_likelihood.x)\n",
    "print('Standard errors: %s. ' % np.round(standard_errors, 3))\n",
    "\n",
    "# Use LogisticRegression for comparison\n",
    "model_logistic = LogisticRegression(penalty = None)\n",
    "model_logistic.fit(x_train.reshape(-1, 1), y_train)\n",
    "\n",
    "\n",
    "print('Estimated intercept')\n",
    "print('LogisticRegression(): %3.3f. Our : %3.3f. True value: %3.3f' % (model_logistic.intercept_, res_optim_likelihood.x[0], beta0))\n",
    "print('Estimated slope')\n",
    "print('LogisticRegression(): %3.3f. Our : %3.3f. True value: %3.3f' % (model_logistic.coef_, res_optim_likelihood.x[1], beta1))\n",
    "\n",
    "print('Predicting first 10 test observations')\n",
    "print('LogisticRegression()')\n",
    "print(model_logistic.predict_proba(x_test.reshape(-1, 1))[:10, :])\n",
    "print('Without a package')\n",
    "beta_hat = res_optim_likelihood.x\n",
    "X_test = np.vstack((np.ones(200), x_test)).T\n",
    "z = np.dot(X_test, beta_hat)\n",
    "Pr_y_1_hat = 1/(1 + np.exp(-z))\n",
    "Pr_y_0_hat = 1 - Pr_y_1_hat\n",
    "print(np.vstack((Pr_y_0_hat, Pr_y_1_hat)).T[:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above calls the function `minimize` to carry out the numerical optimisation. The function requires a starting value of the parameter vector (`beta_optim_start`), which method to use (`BFGS`, the Broyden–Fletcher–Goldfarb–Shanno method is an iterative method for unconstrained nonlinear optimisation), an optional gradient (`jac`, Jacobian, gradient of a vector valued function). The `options` attribute specifies the details of the optimisation: `gtol` specifies the tolerance level for the norm of the gradient to determine convergence (at the minimum $\\widehat{\\boldsymbol{\\beta}}, \\nabla_{\\boldsymbol{\\beta}}\\ell(\\widehat{\\boldsymbol{\\beta}}|\\mathbf{y}) = \\mathbf{0}$), while `maxiter` determines the maximum number of iterations and `disp` controls the display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Improving the optimisation using gradient information\n",
    "The optimisation above can be made more efficient by utilising the gradient of the objective function that can be supplied via the `jac` attribute. Without supplying a gradient, the optimiser computes the gradient by finite differences which can be computationally costly and inaccurate, especially if $\\boldsymbol{\\beta}$ is high-dimensional. An alternative is to code the gradient analytically ourselves. Even better (at least less tedious!) is to use automatic differentiation. The `autograd` package provides functions for computing the gradient and the Hessian using automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: autograd in c:\\users\\144052\\appdata\\roaming\\python\\python311\\site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from autograd) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#FBEFFB;\"><p style=\"font-size:20px;color:#FF0080\">&#9888; Beware!</p> <!--- Warning --->\n",
    "\n",
    "<p>If the function we want to compute the gradient of contains any <code>numpy</code> or <code>scipy</code> functions, it is important that these come from autograd's version of <code>numpy</code> and <code>scipy</code>. See below for an illustration..</p> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import grad, hessian # Automatic differentiation functions\n",
    "import autograd.numpy as np_autograd # Important to use autograd's numpy and scipy if we want to take derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows that computing the gradient using automatic differentiation does not work if the function uses the standard `numpy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type ArrayBox which has no callable exp method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'ArrayBox' object has no attribute 'exp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_89225/206555760.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgrad_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_likelihood_logistic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# The second argument indicates which argument we compute the gradient with respect to.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgrad_beta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbeta0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/autograd/wrap_util.py\u001b[0m in \u001b[0;36mnary_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0munary_operator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munary_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnary_op_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnary_op_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnary_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnary_operator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/autograd/differential_operators.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0marguments\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0minstead\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     should be scalar-valued. The gradient has the same type as the argument.\"\"\"\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mvjp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         raise TypeError(\"Grad only applies to real scalar-output functions. \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/autograd/core.py\u001b[0m in \u001b[0;36mmake_vjp\u001b[0;34m(fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstart_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVJPNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mend_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_node\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mend_node\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/autograd/tracer.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(start_node, fun, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mstart_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mend_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_box\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mend_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstart_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mend_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/autograd/wrap_util.py\u001b[0m in \u001b[0;36munary_f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0msubargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubvals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msubargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_89225/4022065715.py\u001b[0m in \u001b[0;36mlog_likelihood_logistic\u001b[0;34m(beta, y, X)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlog_Pr_y_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# log Pr(y = 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mlog_Pr_y_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# log Pr(y = 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type ArrayBox which has no callable exp method"
     ]
    }
   ],
   "source": [
    "grad_beta = grad(log_likelihood_logistic, 0) # The second argument indicates which argument we compute the gradient with respect to.\n",
    "grad_beta(np.array([beta0, beta1]), y_train, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses the `numpy` library from `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.82643604  1.7372991 ]\n"
     ]
    }
   ],
   "source": [
    "def log_likelihood_logistic_autograd(beta, y, X):\n",
    "    \"\"\"\n",
    "    Log-likelihood for the logistic model.\n",
    "    \"\"\"\n",
    "    z = np_autograd.dot(X, beta)\n",
    "    log_Pr_y_1 = -np_autograd.log1p(np_autograd.exp(-z[y == 1])) # log Pr(y = 1)\n",
    "    log_Pr_y_0 = -z[y == 0] - np_autograd.log1p(np_autograd.exp(-z[y == 0])) # log Pr(y = 0)\n",
    "    \n",
    "    return np_autograd.sum(log_Pr_y_1) + np_autograd.sum(log_Pr_y_0)\n",
    "\n",
    "# Try computing the gradient\n",
    "grad_beta = grad(log_likelihood_logistic_autograd, 0) # The second argument indicates which argument we compute the gradient with respect to.\n",
    "print(grad_beta(np.array([beta0, beta1]), y_train, X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code estimates the parameters by incorporating the gradient in the numerical optimisation. The optimisation is slightly more efficient compared to the one above that did not incorporate gradients and the differences will be more pronounced when there are more parameters. Finally, the code also computes asymptotically valid confidence intervals using (i) the `hess_inv` from `scipy.optimize` (as above) and (ii) using the automatic differentiation package. Note that none of these were possible using the `LogisticRegression()` function because the package does not provide standard errors. This is one example of when a package does not provide everything of interest and it is useful to be able to code it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 316.011108\n",
      "         Iterations: 12\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "      fun: 316.01110833735123\n",
      " hess_inv: array([[ 0.12023962, -0.16922032],\n",
      "       [-0.16922032,  0.25881259]])\n",
      "      jac: array([-8.39198237e-06, -6.43864505e-06])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 13\n",
      "      nit: 12\n",
      "     njev: 13\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([-4.96727905,  7.19560152])\n",
      "Estimated intercept\n",
      "LogisticRegression(): -4.967. Our : -4.967. True value: -4.500\n",
      "Estimated slope\n",
      "LogisticRegression(): 7.195. Our : 7.196. True value: 6.500\n",
      "Predicting first 10 test observations\n",
      "LogisticRegression()\n",
      "[[0.49370842 0.50629158]\n",
      " [0.97654598 0.02345402]\n",
      " [0.09728647 0.90271353]\n",
      " [0.10596371 0.89403629]\n",
      " [0.48021172 0.51978828]\n",
      " [0.97508617 0.02491383]\n",
      " [0.98382906 0.01617094]\n",
      " [0.19546666 0.80453334]\n",
      " [0.21145111 0.78854889]\n",
      " [0.19294399 0.80705601]]\n",
      "Without a package\n",
      "[[0.49365908 0.50634092]\n",
      " [0.9765461  0.0234539 ]\n",
      " [0.09725871 0.90274129]\n",
      " [0.10593425 0.89406575]\n",
      " [0.48016172 0.51983828]\n",
      " [0.97508621 0.02491379]\n",
      " [0.98382946 0.01617054]\n",
      " [0.19542384 0.80457616]\n",
      " [0.2114066  0.7885934 ]\n",
      " [0.19290146 0.80709854]]\n",
      "Standard errors autodiff: [0.347 0.511]. \n",
      "Standard errors scipy.optimize: [0.346 0.508]. \n",
      "Approximate 95% confidence intervals\n",
      "[-5.648  6.194]\n",
      "[-4.287  8.198]\n"
     ]
    }
   ],
   "source": [
    "neg_log_likelihood_autograd = lambda beta: -log_likelihood_logistic_autograd(beta, y_train, X_train)\n",
    "grad_beta = grad(neg_log_likelihood_autograd)\n",
    "\n",
    "beta_optim_start = np.zeros(2)\n",
    "res_optim_likelihood = minimize(neg_log_likelihood_autograd, beta_optim_start, method='BFGS', jac = grad_beta, \n",
    "                                options={'gtol': 1e-04, 'maxiter': 1000, 'disp': True})\n",
    "\n",
    "print(res_optim_likelihood)\n",
    "\n",
    "print('Estimated intercept')\n",
    "print('LogisticRegression(): %3.3f. Our : %3.3f. True value: %3.3f' % (model_logistic.intercept_, res_optim_likelihood.x[0], beta0))\n",
    "print('Estimated slope')\n",
    "print('LogisticRegression(): %3.3f. Our : %3.3f. True value: %3.3f' % (model_logistic.coef_, res_optim_likelihood.x[1], beta1))\n",
    "\n",
    "print('Predicting first 10 test observations')\n",
    "print('LogisticRegression()')\n",
    "print(model_logistic.predict_proba(x_test.reshape(-1, 1))[:10, :])\n",
    "print('Without a package')\n",
    "beta_hat = res_optim_likelihood.x\n",
    "X_test = np.vstack((np.ones(200), x_test)).T\n",
    "z = np.dot(X_test, beta_hat)\n",
    "Pr_y_1_hat = 1/(1 + np.exp(-z))\n",
    "Pr_y_0_hat = 1 - Pr_y_1_hat\n",
    "print(np.vstack((Pr_y_0_hat, Pr_y_1_hat)).T[:10, :])\n",
    "\n",
    "# Finally, standard errors for approximate confidence intervals.\n",
    "hess_beta = hessian(neg_log_likelihood_autograd)\n",
    "standard_errors_autodiff = np.sqrt(np.diag(np.linalg.inv(hess_beta(res_optim_likelihood.x))))\n",
    "print('Standard errors autodiff: %s. ' % np.round(standard_errors_autodiff, 3))\n",
    "print('Standard errors scipy.optimize: %s. ' % np.round(standard_errors, 3))\n",
    "\n",
    "print('Approximate 95% confidence intervals')\n",
    "lower_CI = res_optim_likelihood.x - 1.96*standard_errors_autodiff\n",
    "upper_CI = res_optim_likelihood.x + 1.96*standard_errors_autodiff\n",
    "print(np.round(lower_CI, 3))\n",
    "print(np.round(upper_CI, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient-based optimisation without packages\n",
    "Finally, we showcase how to do optimisation without packages using simple optimisation algorithms. As an illustrative example, we use a Poisson regression.\n",
    "\n",
    "The Poisson regression model is $$p(y_i|\\beta,x_i)= \\frac{\\mu_i^{y_i}\\exp(-\\mu_i)}{y_i!}, y_i \\in \\mathbb{Z}^+,$$\n",
    "where $\\mu_i = \\exp(x_i^\\top \\beta) \\geq 0$. Note that $\\mathrm{E}(y_i|\\beta,x_i)=\\mu_i$. The following codes simulates and plots data from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "true_params = np.array([2.5, 0.7])\n",
    "X = np.hstack((np.ones((n, 1)), np.linspace(0, 3, n).reshape(-1, 1)))\n",
    "y = sps.poisson.rvs(np.exp(np.dot(X, true_params))) # Model: y_i \\sim Poisson(exp(beta_1*x_i1 + beta_2*x_i2)), first x_i1 = 1\n",
    "plt.plot(X[:, 1], y, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood based on $\\mathbf{y}=(y_1, \\dots, y_n)$ independent observations is \n",
    "$$\\ell(\\beta|\\mathbf{y}) = \\sum_{i=1}^n \\log p(y_i|\\beta,x_i) = \\sum_{i=1}^n {y_i}\\log(\\mu_i)-\\mu_i-\\log(y_i!).$$\n",
    "The following code implements the log-likelihood of the model. Note that the code uses `np_autograd` as imported above since we will compute the gradient using automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import factorial\n",
    "def log_likelihood_function(beta, X, y):\n",
    "    mu = np_autograd.exp(np_autograd.dot(X, beta))\n",
    "    return(np_autograd.sum(y*np_autograd.log(mu) - mu - np_autograd.log(factorial(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the MLE using gradient ascent. Starting from $\\beta^{(0)}$, we iterate\n",
    "$$\\beta^{(j)} = \\beta^{(j-1)} + \\gamma \\nabla \\ell(\\beta^{(j-1)}|\\mathbf{y}) $$\n",
    "where $\\gamma > 0$ is the so-called learning rate, until some convergence criteria is met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradient and Hessian by automatic differentiation:\n",
    "grad_beta = grad(log_likelihood_function, 0)\n",
    "Hess_beta = hessian(log_likelihood_function, 0)\n",
    "# Gradient ascent method. Go in the direction of the gradient\n",
    "beta_old = np.array([0, 0]) # Start value\n",
    "\n",
    "gamma = 0.000001  # Learning rate\n",
    "precision = 0.00001  # Desired precision of result\n",
    "max_iters = 10000  # Maximum number of iterations\n",
    "   \n",
    "for i in range(max_iters):\n",
    "    beta_new = beta_old + gamma * grad_beta(beta_old, X, y)\n",
    "    step = beta_new - beta_old\n",
    "    #print(beta_new)\n",
    "    if np.linalg.norm(step) <= precision:\n",
    "        break\n",
    "    beta_old = beta_new\n",
    "\n",
    "if i == (max_iters - 1):\n",
    "    print(\"Careful as optimiser has not met termination criteria\")\n",
    "    print(\"Last iteration %s\" % beta_new)\n",
    "else:   \n",
    "    print(\"Maximum achieved in %s iterations\" % (i+1))   \n",
    "    print(\"Maximum Likelihood Estimator\", beta_new)\n",
    "    print(\"Variance covariance matrix\")\n",
    "    print(-np.linalg.inv(Hess_beta(beta_new, X, y)))\n",
    "    print(\"True parameters\", true_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Second order optimisation without packages\n",
    "It can be shown that if the learning rate mimics the local curvature (given by the Hessian) of the objective function, then the optimisation achieves optimal convergence rate. This happens when the learning rate is multiplied by $-\\left(\\nabla \\nabla^\\top \\ell(\\beta|y)\\right)^{-1}$, in case this is known as Newton's method, or Newton-Raphson method. Such methods are referred to as second-order method. The gradient-based method in the previous section is referred to as first-order optimisation.\n",
    "\n",
    "The following code implements Newton's method. It can be seen that it converges much faster than the above first order optimisation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative (faster) optimiser using Newton-Raphson. Adaptive (to the curvature of the function) step size multiplier\n",
    "beta_old = np.zeros(2)\n",
    "gamma = 1\n",
    "precision = 0.00001  # Desired precision of result\n",
    "max_iters = 10000  # Maximum number of iterations\n",
    "   \n",
    "for i in range(max_iters):\n",
    "    beta_new = beta_old + gamma*np.linalg.solve(-Hess_beta(beta_old, X, y),  grad_beta(beta_old, X, y))\n",
    "    step = beta_new - beta_old\n",
    "    if np.linalg.norm(step) <= precision:\n",
    "        break\n",
    "    beta_old = beta_new\n",
    "\n",
    "if i == (max_iters - 1):\n",
    "    print(\"Careful as optimiser has not met termination criteria\")\n",
    "else:   \n",
    "    print(\"Maximum achieved in %s iterations\" % (i+1))   \n",
    "    print(\"Maximum Likelihood Estimator\", beta_new)\n",
    "    print(\"Variance covariance matrix\")\n",
    "    print(-np.linalg.inv(Hess_beta(beta_new, X, y)))\n",
    "    print(\"True parameters\", true_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<hr style=\"height:5px;border:none;color:#333;background-color:#333;\" />\n",
    "<h1> Practice, Practice, Practice </h1>\n",
    "\n",
    "<ol>\n",
    "<li>Go through the notebook again and check that you have understood everything covered.</li><br>\n",
    "\n",
    "\n",
    "<li> Do the tutorial problems that you can find on Canvas.</li><br>\n",
    "</ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Material by Matias Quiroz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
